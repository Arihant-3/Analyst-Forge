### Answer

1. Done

2. Done

3. Done

4.  - A is significantly better
    - NO 

---

- **What is the p-value and what does it mean?**  
It's the chance of seeing your result if there's no true difference.  


- **Is the sample size large enough?**  
No, the sample size is likely not large enough to detect a small difference with high confidence.

- **Would you recommend launching the new version?**  
Result: The difference is NOT statistically significant (p â‰¥ 0.05).
Recommendation: Stick to current version or collect more data.

---
---

# ğŸ§ª A/B Testing â€” Complete Notes

## ğŸ“˜ What is A/B Testing?
A/B testing is a controlled experiment comparing two versions (A and B) to determine which performs better on a key metric (e.g., conversion rate, click-through rate, revenue per user, etc.).

---

## ğŸ”¤ Key Terminology

- **Group A (Control)**: The current or original version.
- **Group B (Variant)**: The new or modified version being tested.
- **Conversion**: A user completing the desired action (e.g., purchase, sign-up).
- **Conversion Rate**: Conversions / Total Visitors.
- **Null Hypothesis (Hâ‚€)**: Assumes no difference between A and B.
- **Alternative Hypothesis (Hâ‚)**: Assumes a meaningful difference between A and B.
- **P-value**: Probability of observing the result (or more extreme) under Hâ‚€.
- **Alpha (Î±)**: The threshold for significance (commonly 0.05).
- **Effect Size**: Magnitude of the difference between A and B.
- **Power**: Probability of detecting a true effect (commonly 80%).

---

## ğŸ”¬ Statistical Testing for Conversion Rates

### âœ… Use Case:
Use **Two-Proportion Z-Test** when:
- You are comparing **two groups**.
- The outcome is **binary** (e.g., converted: yes/no).
- You want to test **proportions** (e.g., 10% vs 13%).

### ğŸ§ª Hypotheses
- **Hâ‚€**: Conversion_A = Conversion_B
- **Hâ‚**: Conversion_A â‰  Conversion_B (two-sided)

### ğŸ“‰ Z-Test for Proportions (Python Example)
```python
from statsmodels.stats.proportion import proportions_ztest

count = [converted_A, converted_B]      # number of conversions
nobs = [total_A, total_B]               # total users in each group

z_stat, p_val = proportions_ztest(count=count, nobs=nobs)
```

---

## ğŸ“Œ P-Value Interpretation

- **p < 0.05** â†’ Statistically significant â†’ Reject Hâ‚€ â†’ Likely a real difference.
- **p â‰¥ 0.05** â†’ Not statistically significant â†’ Not enough evidence to reject Hâ‚€.

âš ï¸ A non-significant result doesn't mean B is worse â€” it could mean **not enough data**.

---

## ğŸ”¢ Calculating Effect Size

$
\text{Effect Size} = \text{Conversion}_B - \text{Conversion}_A
$

Even if the effect is small (e.g., 3%), it might still be **practically significant** depending on business goals.

---

## ğŸ¯ Sample Size & Power

To detect a meaningful difference with:
- **Î± = 0.05** (confidence level = 95%)
- **Power = 0.80** (80% chance of detecting a real effect)
- Small effect (e.g., 3%)

ğŸ”¢ You typically need **>1,000 users per group**

Use tools like:
- [Evan Miller's Sample Size Calculator](https://www.evanmiller.org/ab-testing/sample-size.html)

---

## ğŸ“Š Visualization (Optional)

```python
import seaborn as sns
import matplotlib.pyplot as plt

sns.barplot(data=df, x='Group', y='Converted', ci=95)
plt.title("Conversion Rate by Group (95% CI)")
plt.ylim(0, 0.2)
plt.show()
```

---

## âœ… Decision Checklist

| Question                          | Yes/No |
|----------------------------------|--------|
| Did you define a clear hypothesis?         | âœ…    |
| Is the sample size large enough?          | âœ…/âŒ |
| Is the p-value < Î± (e.g., 0.05)?           | âœ…/âŒ |
| Is the effect size practically significant?| âœ…/âŒ |
| Did you check power before starting?       | âœ…/âŒ |

---

## âš ï¸ Common Mistakes

- Stopping test too early (before enough data)
- Interpreting p > 0.05 as "no difference"
- Not accounting for multiple testing
- Ignoring practical/business significance

---

## ğŸ§  Pro Tips

- Always predefine: goal, hypothesis, metric, alpha, and sample size.
- Visualize results with confidence intervals.
- Consider **Bayesian A/B testing** if sample size is small or you want probability-based insights.

---

## ğŸ“š Resources

- [Statsmodels Documentation](https://www.statsmodels.org/)
- [Optimizely A/B Testing Guide](https://www.optimizely.com/)
- [Evan Miller's Tools](https://www.evanmiller.org/)

---
